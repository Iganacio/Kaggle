{"metadata":{"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\nexample = pd.read_csv(\"/kaggle/input/titanic/gender_submission.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# A first glance","metadata":{}},{"cell_type":"code","source":"#Nan\nprint(train.isna().sum())\nprint(test.isna().sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualization","metadata":{}},{"cell_type":"code","source":"#Dataframe\n\nprint(train.columns.values.tolist())\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's review each variable with graphs and see which one may be more useful.","metadata":{}},{"cell_type":"markdown","source":"## Pclass","metadata":{}},{"cell_type":"code","source":"#Data\nVariable=\"Pclass\"\n\nprint(train[Variable].unique())\nprint(train[Variable].value_counts())\n\nsns.catplot(x = Variable, y = 'Survived',\n            data = train, kind = 'bar',height=6, aspect=2,margin_titles = True)\n\nplt.title('Survival Probability ' + Variable, fontsize = (13))\nplt.ylabel('Survival Probability')\nplt.show()\n\nsns.catplot(x = Variable, y = 'Survived', hue = 'Sex',\n            data = train, kind = 'bar',height=6, aspect=2,margin_titles = True)\n\nplt.title('Survival Probability by Sex and ' + Variable, fontsize = (13))\nplt.ylabel('Survival Probability')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No surprises in the first graph: first class people have a higher survival rate than the second class and the second. The same phenomenon happends between the second and third class.\nWomen in first and second class have a similar survival rate. The same phenomenon happens between men in the second and third class.","metadata":{}},{"cell_type":"markdown","source":"## Name\nAt the begging I directly assume that the name hasn't influence in the survival rate, but after a close examination a discover a important detail.","metadata":{}},{"cell_type":"code","source":"train['Name_bin'] = train['Name'].str.extract('( [A-Z]+\\w*\\.)', expand=False).str.strip()\n#train['Name_bin'] = np.where(train['Name_bin'].isin(['Mr.','Miss.','Mrs.','Master.',\"Don.\",\"Rev.\",\"Dr.\"]), train['Name_bin'], 'somebodysomebod')\n\ntrain['Name_bin'] = np.where(train['Name_bin'] == \"Mme.\",\"Mrs.\",train['Name_bin'])\ntrain['Name_bin'] = np.where(train['Name_bin'] == \"Don.\",\"Mr.\",train['Name_bin'])\ntrain['Name_bin'] = np.where(train['Name_bin'] == \"Mlle.\",\"Miss.\",train['Name_bin'])\ntrain['Name_bin'] = np.where(train['Name_bin'] == \"Ms.\",\"Mrs.\",train['Name_bin'])\n\ntrain['Name_bin'] = np.where(train['Name_bin'] == \"Major.\",\"Military\",train['Name_bin'])\ntrain['Name_bin'] = np.where(train['Name_bin'] == \"Col.\",\"Military\",train['Name_bin'])\ntrain['Name_bin'] = np.where(train['Name_bin'] == \"Capt.\",\"Military\",train['Name_bin'])\n\ntrain['Name_bin'] = np.where(train['Name_bin'] == \"Master.\",\"Proffesion\",train['Name_bin'])\ntrain['Name_bin'] = np.where(train['Name_bin'] == \"Dr.\",\"Proffesion\",train['Name_bin'])\ntrain['Name_bin'] = np.where(train['Name_bin'] == \"Rev.\",\"Proffesion\",train['Name_bin'])\n\ntrain['Name_bin'] = np.where(train['Name_bin'] == \"Sir.\",\"Royal\",train['Name_bin'])\ntrain['Name_bin'] = np.where(train['Name_bin'] == \"Lady.\",\"Royal\",train['Name_bin'])\ntrain['Name_bin'] = np.where(train['Name_bin'] == \"Countess.\",\"Royal\",train['Name_bin'])\ntrain['Name_bin'] = np.where(train['Name_bin'] == \"Jonkheer.\",\"Royal\",train['Name_bin'])\n\n\n\n\npd.set_option('display.max_rows', None)\n\nprint(train)#.value_counts())\n\ndef plot_bar(dataframe,colname):\n    survived = dataframe[dataframe['Survived']==1][colname].value_counts()\n    dead = dataframe[dataframe['Survived']==0][colname].value_counts()\n    result = pd.DataFrame([survived,dead], index=['Survived','dead'])\n    return result\n\nplt.figure(figsize=(7,6))\nplot_bar(train,'Name_bin').plot(kind='bar')\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After this graph I decide to merge some categories, and do the same transformation for the test data:","metadata":{}},{"cell_type":"code","source":"train['Name_bin'] = np.where(train['Name_bin'] == \"Royal\",\"Other\",train['Name_bin'])\ntrain['Name_bin'] = np.where(train['Name_bin'] == \"Proffesion\",\"Other\",train['Name_bin'])\ntrain['Name_bin'] = np.where(train['Name_bin'] == \"Military\",\"Other\",train['Name_bin'])\n\nplt.figure(figsize=(7,6))\nplot_bar(train,'Name_bin').plot(kind='bar')\nplt.show()\n\ntest['Name_bin'] = test['Name'].str.extract('( [A-Z]+\\w*\\.)', expand=False).str.strip()\ntest['Name_bin'] = np.where(test['Name_bin'].isin(['Mr.','Miss.','Mrs.']), test['Name_bin'], 'Other')\nprint(train['Name_bin'].value_counts())\nprint(test['Name_bin'].value_counts())\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Sex","metadata":{}},{"cell_type":"code","source":"#Data\nVariable=\"Sex\"\n\nprint(train[Variable].unique())\nprint(train[Variable].value_counts())\n\nsns.catplot(x = Variable, y = 'Survived',\n            data = train, kind = 'bar',height=6, aspect=2,margin_titles = True)\n\nplt.title('Survival Probability ' + Variable, fontsize = (13))\nplt.ylabel('Survival Probability')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is one of the most important variables. In fact one model in which all women survived and all men died has an acuraccy of 0.7655.","metadata":{}},{"cell_type":"markdown","source":"## Age\n\nA classification is necessary to be able to see the data more clearly. I am going to use bins to classificate them","metadata":{}},{"cell_type":"code","source":"Variable=\"Age\"\n\ntrain[\"Baby\"] = np.where(train[\"Age\"] <5,1,0)\ntrain[\"Children\"] = np.where((train[\"Age\"] >=5) & (train[\"Age\"] <12),1,0)\ntrain[\"Teeneger\"] = np.where((train[\"Age\"] >=12) & (train[\"Age\"] <20),1,0)\ntrain[\"Young\"] = np.where((train[\"Age\"] >=20) & (train[\"Age\"] <30),1,0)\ntrain[\"Adult\"] = np.where((train[\"Age\"] >=30) & (train[\"Age\"] <50),1,0)\ntrain[\"Senior\"] = np.where(train[\"Age\"] >50,1,0)\n\ntest[\"Baby\"] = np.where(test[\"Age\"] <5,1,0)\ntest[\"Children\"] = np.where((test[\"Age\"] >=5) & (test[\"Age\"] <12),1,0)\ntest[\"Teeneger\"] = np.where((test[\"Age\"] >=12) & (test[\"Age\"] <20),1,0)\ntest[\"Young\"] = np.where((test[\"Age\"] >=20) & (test[\"Age\"] <30),1,0)\ntest[\"Adult\"] = np.where((test[\"Age\"] >=30) & (test[\"Age\"] <50),1,0)\ntest[\"Senior\"] = np.where(test[\"Age\"] >50,1,0)\n\n\nVariable=\"Baby\"\nsns.catplot(x = Variable, y = 'Survived',\n            data = train, kind = 'bar',height=2, aspect=2,margin_titles = True)\n\nVariable=\"Children\"\nsns.catplot(x = Variable, y = 'Survived',\n            data = train, kind = 'bar',height=2, aspect=2,margin_titles = True)\n\nVariable=\"Teeneger\"\nsns.catplot(x = Variable, y = 'Survived',\n            data = train, kind = 'bar',height=2, aspect=2,margin_titles = True)\n\nVariable=\"Young\"\nsns.catplot(x = Variable, y = 'Survived',\n            data = train, kind = 'bar',height=2, aspect=2,margin_titles = True)\n\nVariable=\"Adult\"\nsns.catplot(x = Variable, y = 'Survived',\n            data = train, kind = 'bar',height=2, aspect=2,margin_titles = True)\n\nVariable=\"Senior\"\nsns.catplot(x = Variable, y = 'Survived',\n            data = train, kind = 'bar',height=2, aspect=2,margin_titles = True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Conclusion: There are not linear efects in age.","metadata":{}},{"cell_type":"markdown","source":"## SibSp","metadata":{}},{"cell_type":"code","source":"#Data\nVariable=\"SibSp\"\n\nprint(train[Variable].unique())\nprint(train[Variable].value_counts())\n\nsns.catplot(x = Variable, y = 'Survived',\n            data = train, kind = 'bar',height=6, aspect=2,margin_titles = True)\n\nplt.title('Survival Probability ' + Variable, fontsize = (13))\nplt.ylabel('Survival Probability')\nplt.show()\n\nsns.catplot(x = Variable, y = 'Survived', hue = 'Sex',\n            data = train, kind = 'bar',height=6, aspect=2,margin_titles = True)\n\nplt.title('Survival Probability by Sex and ' + Variable, fontsize = (13))\nplt.ylabel('Survival Probability')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"There are no linear efect related with this feature.","metadata":{}},{"cell_type":"code","source":"#Data\nVariable=\"Parch\"\n\nprint(train[Variable].unique())\nprint(train[Variable].value_counts())\n\nsns.catplot(x = Variable, y = 'Survived',\n            data = train, kind = 'bar',height=6, aspect=2,margin_titles = True)\n\nplt.title('Survival Probability ' + Variable, fontsize = (13))\nplt.ylabel('Survival Probability')\nplt.show()\n\nsns.catplot(x = Variable, y = 'Survived', hue = 'Sex',\n            data = train, kind = 'bar',height=6, aspect=2,margin_titles = True)\n\nplt.title('Survival Probability by Sex and ' + Variable, fontsize = (13))\nplt.ylabel('Survival Probability')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are no linear efect related with this feature.","metadata":{}},{"cell_type":"markdown","source":"## Ticket\nI directly assume that the number of the ticket hasn't influence in the survival rate.","metadata":{}},{"cell_type":"markdown","source":"## Fare","metadata":{}},{"cell_type":"code","source":"Variable=\"Fare\"\n\nprint(train[Variable].unique())\nprint(train[Variable].value_counts())\n\ntrain[\"Fare\"] = np.where(train[\"Fare\"] >300,300,train[\"Fare\"])\ntest[\"Fare\"] = np.where(test[\"Fare\"] >300,300,test[\"Fare\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With this distribution of data I am going to log normalize the distribution but first I have to complete the Nan values in the test set with the most common value in the train set (8.0500). Also I have eliminated the outlayers.","metadata":{}},{"cell_type":"markdown","source":"## Cabin","metadata":{}},{"cell_type":"markdown","source":"Hypothesis: the letter of the cabin is related to the location inside the ship and therefore to the survival rate.","metadata":{}},{"cell_type":"code","source":"#Data\nVariable=\"Letter\"\n\ntrain[\"Cabin\"] = train[\"Cabin\"].fillna(\"Z\")\ntrain[Variable]=train[\"Cabin\"].astype(str).str[0]\n\nprint(train[Variable].unique())\nprint(train[Variable].value_counts())\n\nsns.catplot(x = Variable, y = 'Survived',\n            data = train, kind = 'bar',height=6, aspect=2,margin_titles = True)\n\nplt.title('Survival Probability ' + Variable, fontsize = (13))\nplt.ylabel('Survival Probability')\nplt.show()\n\n\nsns.catplot(x = Variable, y = 'Survived', hue = 'Pclass',\n            data = train, kind = 'bar',height=6, aspect=2,margin_titles = True)\n\nplt.title('Survival Probability by Sex and ' + Variable, fontsize = (13))\nplt.ylabel('Survival Probability')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Most of the people has unkonwn cabin (\"Z\"). That make the this analysis impractical.","metadata":{}},{"cell_type":"markdown","source":"## Embarked","metadata":{}},{"cell_type":"markdown","source":"Hypothesis: The embarkation site is related to the location inside the ship and therefore to the survival rate.","metadata":{}},{"cell_type":"code","source":"#Data\nVariable=\"Embarked\"\n\nprint(train[Variable].unique())\nprint(train[Variable].value_counts())\n\nsns.catplot(x = Variable, y = 'Survived',\n            data = train, kind = 'bar',height=6, aspect=2,margin_titles = True)\n\nplt.title('Survival Probability ' + Variable, fontsize = (13))\nplt.ylabel('Survival Probability')\nplt.show()\n\nsns.catplot(x = Variable, y = 'Survived', hue = 'Sex',\n            data = train, kind = 'bar',height=6, aspect=2,margin_titles = True)\n\nplt.title('Survival Probability by Sex and ' + Variable, fontsize = (13))\nplt.ylabel('Survival Probability')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I can try to include this variable and see the results.","metadata":{}},{"cell_type":"code","source":"#One hot encoding of \"Sex\", \"Parch\" and \"Embarked\"\nSex_train=pd.get_dummies(train[\"Sex\"],prefix='Sex')\nParch_train=pd.get_dummies(train[\"Parch\"],prefix='Parch')\nEmbarked_train=pd.get_dummies(train[\"Embarked\"],prefix='Embarked')\nClass_train=pd.get_dummies(train[\"Pclass\"],prefix='Pclass')\nNames=pd.get_dummies(train[\"Name_bin\"],prefix='Name')\n\ntrain = pd.concat([train, Sex_train,Parch_train,Embarked_train,Class_train,Names], axis=1)\n\nSex_test=pd.get_dummies(test[\"Sex\"],prefix='Sex')\nParch_test=pd.get_dummies(test[\"Parch\"],prefix='Parch')\nEmbarked_test=pd.get_dummies(test[\"Embarked\"],prefix='Embarked')\nClass_test=pd.get_dummies(test[\"Pclass\"],prefix='Pclass')\nNames=pd.get_dummies(test[\"Name_bin\"],prefix='Name')\n\ntest = pd.concat([test, Sex_test,Parch_test,Embarked_test,Class_test,Names], axis=1)\n\n\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Normalize Fare\ntrain[\"Fare\"] = train[\"Fare\"].fillna(8.0500)\ntest[\"Fare\"] = test[\"Fare\"].fillna(8.0500)\n\ntrain[\"Fare\"]=train[\"Fare\"]+1\ntest[\"Fare\"]=test[\"Fare\"]+1\n\ntrain['log_Fare'] = np.log(train[\"Fare\"])\ntest['log_Fare'] = np.log(test[\"Fare\"])\n\nMax_Fare=max([test['log_Fare'].abs().max(),train['log_Fare'].abs().max()])\nprint(Max_Fare)\n\ntrain[\"Normalized_Fare\"] = train['log_Fare']/Max_Fare\n\ntest[\"Normalized_Fare\"] = train['log_Fare']/Max_Fare\n\ntrain.head()\n\nplt.figure()\n_ = plt.hist(test[\"Normalized_Fare\"], bins=100)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As the plot shows 'Fare' have now a much nicer distribution.","metadata":{}},{"cell_type":"code","source":"#Normalize Age\n#test[\"Age\"] = test[\"Age\"].fillna(25)\n#train[\"Age\"] = train[\"Age\"].fillna(25)\n\ntrain[\"Age\"] = train[\"Age\"].fillna(25)\ntest[\"Age\"] = test[\"Age\"].fillna(25)\n\nMax_Age=max([test[\"Age\"].abs().max(),train[\"Age\"].abs().max()])\n\nprint(Max_Age)\n\ntrain[\"Age_Norm\"]=train[\"Age\"]/Max_Age\ntest[\"Age_Norm\"]=test[\"Age\"]/Max_Age\n\nprint(train.isna().sum())\nprint(test.isna().sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainShow=train.loc[:,['Survived','Sex_female', 'Sex_male', 'Pclass_1', 'Pclass_2', 'Pclass_3',\"Parch\",\"SibSp\",\n                       'Embarked_C', 'Embarked_Q', 'Embarked_S', 'Normalized_Fare',\"Age_Norm\",'Name_Mr.','Name_Miss.','Name_Mrs.','Name_Other']]\nsns.heatmap(trainShow.corr(),annot=True,cmap='RdYlGn',linewidths=0.2) #data.corr()-->correlation matrix\nfig=plt.gcf()\nfig.set_size_inches(20,12)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Selected variables\ntrainY=train.loc[:,['Survived']]\ntrainX=train.loc[:,['Sex_female', 'Sex_male', 'Pclass_1', 'Pclass_2', 'Pclass_3',\"Parch\",\"SibSp\",  \n                'Embarked_C', 'Embarked_Q', 'Embarked_S', 'Normalized_Fare',\"Age_Norm\",\n                    'Name_Mr.','Name_Miss.','Name_Mrs.','Name_Other']]\ntestX=test.loc[:,['Sex_female', 'Sex_male', 'Pclass_1', 'Pclass_2', 'Pclass_3',\"Parch\",\"SibSp\", \n                    'Embarked_C', 'Embarked_Q', 'Embarked_S', 'Normalized_Fare',\"Age_Norm\",\n                'Name_Mr.','Name_Miss.','Name_Mrs.','Name_Other']]\n\nprint(trainX.isna().sum())\nprint(testX.isna().sum())\n\nprint(trainX[\"Name_Mr.\"].head())\ntrainX.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MODELING\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\nimport shap\nfrom catboost import Pool\nfrom sklearn.svm import SVC\nfrom catboost import CatBoostClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import StackingClassifier\n\nfrom sklearn.model_selection import RandomizedSearchCV","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PERFORMING 50-FOLD CROSS VALIDATION\n\ndef cross_val(model):\n    \n    cv_scores = []\n    \n    for i in range(0,30):\n        \n        X_train,X_val,y_train,y_val = train_test_split(trainX,trainY,random_state=40, test_size = 0.2, shuffle = True)\n\n        mod = model\n        model = mod.fit(X_train,y_train.values.ravel())\n        mod_pred = model.predict(X_val)\n        mod_score = model.score(X_val,y_val.values.ravel())\n        cv_scores.append(mod_score)\n    return cv_scores\n\n\n\nlogistic_reg = cross_val(LogisticRegression())\nSGD_Class = cross_val(SGDClassifier())\ndecision_tree = cross_val(DecisionTreeClassifier())\nrandom_forest = cross_val(RandomForestClassifier(n_estimators=100))\nSVClass = cross_val(SVC())\nGradBoost = cross_val(GradientBoostingClassifier())\nCatBoost = cross_val(CatBoostClassifier())\n\ncv_scores = pd.DataFrame(columns = ['Logistic_Class','SGDClassifier',\n                                    'DecisionTreeCl','RandomF_Class','SV_Class',\n                                    'GradientBoost','Catboost'])\n\ncv_scores['Logistic_Class'] = logistic_reg\ncv_scores['SGDClassifier'] = SGD_Class\ncv_scores['DecisionTreeCl'] = decision_tree\ncv_scores['RandomF_Class'] = random_forest\ncv_scores['SV_Class'] = SVClass\ncv_scores['GradientBoost'] = GradBoost\ncv_scores['Catboost'] = CatBoost","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv_scores","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CV_means\n\nlogistic_mean = np.mean(logistic_reg)\nSGD_mean = np.mean(SGD_Class)\ndecision_tree_mean = np.mean(decision_tree)\nrandomf_mean = np.mean(random_forest)\nSVC_mean = np.mean(SVClass)\nGradBoost_mean = np.mean(GradBoost)\nCatBoost_mean = np.mean(CatBoost)\n\ncv_means = [logistic_mean,SGD_mean,decision_tree_mean,randomf_mean,SVC_mean,GradBoost_mean,CatBoost_mean]\n\nmeans = pd.DataFrame(cv_scores.columns, columns = ['Algorithms'])\nmeans['cv_means'] = cv_means\nmeans = means.sort_values(by=['cv_means'], ascending=False)\nmeans","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lets use all model at he same time:\n\ndef metamodel(model):\n    \n    model_Y = []\n    \n    for i in range(0,30):\n        \n        X_train,X_val,y_train,y_val = train_test_split(trainX,trainY,random_state=40, test_size = 0.2, shuffle = True)\n\n        mod = model\n        model = mod.fit(trainX,trainY.values.ravel())\n        mod_pred = model.predict_proba(testX)\n    return mod_pred\n\ndef metamodel_2(model):\n    \n    model_Y = []\n    \n    for i in range(0,30):\n        \n        X_train,X_val,y_train,y_val = train_test_split(trainX,trainY,random_state=40, test_size = 0.2, shuffle = True)\n\n        mod = model\n        model = mod.fit(trainX,trainY.values.ravel())\n        mod_pred = model.predict(testX)\n    return mod_pred\n\nlogistic_reg = metamodel(LogisticRegression())\nSGD_Class = metamodel_2(SGDClassifier())\ndecision_tree = metamodel(DecisionTreeClassifier())\nrandom_forest = metamodel(RandomForestClassifier(n_estimators=100))\nSVClass = metamodel_2(SVC())\nGradBoost = metamodel(GradientBoostingClassifier())\nCatBoost = metamodel(CatBoostClassifier())\n\nmodel_Y = pd.DataFrame(columns = ['Logistic_Class',\n                                  'SGDClassifier',\n                                    'DecisionTreeCl','RandomF_Class',\n                                  'SV_Class',\n                                    'GradientBoost','Catboost'])\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_Y['Logistic_Class'] = logistic_reg[:,1]\nmodel_Y['SGDClassifier'] = SGD_Class\nmodel_Y['DecisionTreeCl'] = decision_tree[:,1]\nmodel_Y['RandomF_Class'] = random_forest[:,1]\nmodel_Y['SV_Class'] = SVClass\nmodel_Y['GradientBoost'] = GradBoost[:,1]\nmodel_Y['Catboost'] = CatBoost[:,1]\nmodel_Y['Union'] = (model_Y['Logistic_Class']+model_Y['SGDClassifier'] +model_Y['DecisionTreeCl']+model_Y['RandomF_Class']+\n                    model_Y['SV_Class']+model_Y['GradientBoost']+model_Y['Catboost'])/7\n\nmodel_Y['Union_discrete'] = np.where(model_Y['Union'] > 0.5,1,0)\n\nprint(model_Y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Train accuracy\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\nRFc = RandomForestClassifier(n_estimators = 10, max_features = 8, random_state=0).fit(trainX, trainY.values.ravel())\n\nmodel=LogisticRegression()#LogisticRegression()\nLR=model.fit(trainX, trainY.values.ravel())\n\ny_pred_train = LR.predict(trainX)\n\nconf_mx = confusion_matrix(y_pred_train,trainY.values.ravel())\n\nheat_cm = pd.DataFrame(conf_mx, columns=np.unique(y_pred_train), index = np.unique(y_pred_train))\nheat_cm.index.name = 'Actual'\nheat_cm.columns.name = 'Predicted'\nplt.figure(figsize = (6,5))\nsns.set(font_scale=1.4) # For label size\nsns.heatmap(heat_cm, cmap=\"Blues\", annot=True, annot_kws={\"size\": 16},fmt='g')# font size\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_test = LR.predict(testX)#[:, 1]\n\ntest[\"Survived\"]=y_pred_test\n#test[\"Survived\"]=model_Y['Union_discrete']\nResult=test.loc[:,[\"PassengerId\",'Survived']]\n\n\n#Obtein the file\nResult.to_csv('my_submission.csv', index=False)\n\nprint(Result)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Results\nIn this markdown I'm showing the progres of my model.\n\n### Iteration 0: Baseline\nGoal: My Baseline is provided as an example, in which survivers are selected only by gender. \n + Train accuracy = 0.8922\n + Test accuracy = 0.7655 \n \nConclusion: Whatever model which results are equal or worse than the baseline is almost worthless.\n\n\n\n\n### Iteration 1: Firt simple model\nGoal: In this first model the goal is to create a structure to be improved, take into account some of the variables (Age is not included) and obtain a model to which incremental improvements are added.\n + Train accuracy = 0.8698\n + Test accuracy = 0.7727 \n \nConclusion: It is a very small improvement over the Baseline. The model is still clearly overfitted, but I prefer to work including new variables, as Age, and then work in that direction.\n\n\n\n\n### Iteration 2: Data analysis\nGoal: In this second model the goal is to do a previous treatment of the data that allows us to discard useless data, enhancing the useful ones and simplifying the model.\n + Train accuracy = 0.8720\n + Test accuracy = 0.7608 \n \nConclusion: It is not an improvement over the Baseline. The model is still clearly overfitted. Let's work in the model.\n\n\n\n\n### Iteration 3: Different models\nGoal: Lets try different models and see which one is better. \n + Train accuracy = 0.8307\n + Test accuracy = 0.7631 \n \nConclusion: RandomTree was the best model but still is not an improvement over the Baseline. Let's keep thinking and improving.\n\n\n\n### Iteration 4: All models\nGoal: Lets try different mixing the differente model and see if this is uselful. \n + Train accuracy = 0.8245\n + Test accuracy = 0.7656\n \nConclusion: There is almost an improvement over the Baseline. Let's try with the data again.\n\n\n\n### Iteration 5: New dat analysis\nGoal: Get a better understanding of the data. \n + Train accuracy = 0.8193\n + Test accuracy = 0.7751\n \nConclusion: Best result so far with Logistic Model. Let's try with the data again.\n\n\n\n### Iteration 6: All models\nGoal: Lets try different mixing the differente model and see if this is uselful. \n + Train accuracy = 0.8245\n + Test accuracy = 0.7775\n \nConclusion: Anothner small improvement.\n\n\n\n\n\n","metadata":{}}]}